---
title: "Examples in R* paper"
author: "Ben Lambert"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_notebook
---


```{r}
rm(list=ls())
library(tidyverse)
library(reshape2)
library(rstan)
library(latex2exp)
library(caret)
library(gbm)
library(mvtnorm)
options(mc.cores=4)
rstan_options(auto_write = TRUE)
source("monitornew.R")

caretGrid <- expand.grid(interaction.depth=c(3), n.trees = 50,
                   shrinkage=c(0.1),
                   n.minobsinnode=10)
```

# AR1 example
Functions to generate simulated data
```{r}
f_ar1 <- function(rho, sigma, L){
  x <- vector(length = L)
  x[1] <- rnorm(1, 0, sd=sigma)
  for(i in 2:L)
    x[i] = rho * x[i - 1] + rnorm(1, 0, sd=sigma)
  return(x)
}

f_generate_lower_var_four <- function(var_ratio, rho, sigma, L){
  x <- matrix(nrow = L, ncol = 4)
  for(i in 1:3)
    x[, i] <- f_ar1(rho, sigma, L)
  z <- f_ar1(rho, sigma * sqrt(var_ratio), L)
  x[, 4] <- z
  return(x)
}

f_generate_lower_var_four(1/10, 0.3, 1, 1000) %>% 
  melt() %>% 
  rename(iter=Var1,
         chain=Var2) %>% 
  ggplot(aes(x=value, fill=as.factor(chain))) +
  geom_histogram(position="identity", alpha=0.3)
```


Use gbm model in Caret to predict chain identity
```{r}
# generate data
full_data <- f_generate_lower_var_four(1/3, 0.3, 1, 1000) %>% 
  melt() %>% 
  rename(iter=Var1,
         chain=Var2) %>% 
  dplyr::select(-iter)

# add in extra column of nonsense values since gbm needs 2+ vars
full_data <- full_data %>% 
  mutate(chain=as.factor(chain)) %>% 
  mutate(ones=rnorm(nrow(full_data)))

# create training/testing data
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]

gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid)
# plot(gbmFit1)
plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))

# plot predictions as fn of value
testing_data %>% 
  arrange(value) %>% 
  mutate(predictions=predict(object=gbmFit1, .)) %>% 
  ggplot(aes(x=value, y=predictions)) +
  geom_jitter() +
  coord_flip()
```

Perform 1000 replicates
```{r}
f_replicate <- function(){
  full_data <- f_generate_lower_var_four(1/3, 0.3, 1, 1000) %>% 
  melt() %>% 
  rename(iter=Var1,
         chain=Var2) %>% 
  dplyr::select(-iter)
  rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
  training_data <- full_data[rand_samples, ]
  testing_data <- full_data[-rand_samples, ]
  
  # add in extra column of nonsense values since the method needs at least 2 vars 
  full_data <- full_data %>% 
    mutate(chain=as.factor(chain)) %>% 
    mutate(ones=rnorm(nrow(full_data)))
  training_data <- full_data[rand_samples, ]
  testing_data <- full_data[-rand_samples, ]

  gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)
  plda <- predict(object=gbmFit1, newdata=testing_data)
  a_accuracy <- 
    tibble(predicted=plda, actual=testing_data$chain) %>%
    mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
    summarise(mean(correct)) %>% 
    pull()
  return(a_accuracy)
}

nreplicates <- 1000
accuracies <- vector(length = nreplicates)
for(i in 1:nreplicates){
  if(i%%100==0)
    print(i)
  accuracies[i] <- f_replicate()
}
```

Plot results
```{r}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

g2 <- tibble(accuracy=accuracies/0.25, iteration=seq(1, nreplicates, 1)) %>% 
  ggplot(aes(x=accuracy)) +
  geom_histogram() +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  ggtitle("B.") +
  theme(text = element_text(size=14, colour="black")) +
  xlim(1, NA)

g1 <- testing_data %>% 
  arrange(value) %>% 
  mutate(predictions=predict(object=gbmFit1, .)) %>% 
  ggplot(aes(x=value, y=predictions)) +
  geom_jitter(height = 0.3) +
  xlab("Value") +
  ylab("Chain classification") +
  theme(text = element_text(size=14, colour="black")) +
  coord_flip() +
  ggtitle("A.")
```

Generate R* distribution based on 1000 iterations
```{r}
full_data <- f_generate_lower_var_four(1/3, 0.3, 1, 1000) %>% 
  melt() %>% 
  rename(iter=Var1,
         chain=Var2) %>% 
  dplyr::select(-iter)
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
full_data <- full_data %>% 
    mutate(chain=as.factor(chain)) %>% 
    mutate(ones=rnorm(nrow(full_data)))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]

gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")


nsim <- 1000
mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}

f_generate_all_same_var_four <- function(var_ratio, rho, sigma, L){
  x <- matrix(nrow = L, ncol = 4)
  for(i in 1:4)
    x[, i] <- f_ar1(rho, sigma, L)
  return(x)
}

full_data <- f_generate_all_same_var_four(1/3, 0.3, 1, 1000) %>% 
  melt() %>% 
  rename(iter=Var1,
         chain=Var2) %>% 
  dplyr::select(-iter)
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
full_data <- full_data %>% 
    mutate(chain=as.factor(chain)) %>% 
    mutate(ones=rnorm(nrow(full_data)))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]

gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)
plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

nsim <- 1000
mAccuracy1 <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy1[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}
hist(colMeans(mAccuracy1)/0.25)

mAR1 <- tibble(unconverged=colMeans(mAccuracy)/0.25,
               converged=colMeans(mAccuracy1)/0.25)
g3 <- 
  mAR1 %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8) +
  scale_fill_grey("Series") +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  ggtitle("C.") +
  theme(text = element_text(size=14, colour="black"),
        legend.position = c(0.8, 0.8))

pdf("../output/ar1.pdf", width = 12, height = 6)
multiplot(g1, g2, g3, cols = 3)
dev.off()
```



# Bivariate normal
```{r}
rmvrnorm2D <- function(n, mux, muy, sigmax, sigmay, rho){
  return(rmvnorm(n, c(mux, muy),
                 matrix(c(sigmax^2, sigmax * sigmay * rho,
                          sigmax * sigmay * rho, sigmay^2),
                        ncol = 2)))
}
# calling function
n <- 1000
chain1 <- rmvrnorm2D(n, 0, 0, 1, 1, 0.0) %>% 
  as.data.frame() %>% 
  mutate(chain=1,
         iter=seq_along(chain))
chain2 <- rmvrnorm2D(n, 0, 0, 1, 1, 0.0) %>% 
  as.data.frame() %>% 
  mutate(chain=2,
         iter=seq_along(chain))
chain3 <- rmvrnorm2D(n, 0, 0, 1, 1, 0.0) %>% 
  as.data.frame() %>% 
  mutate(chain=3,
         iter=seq_along(chain))
chain4 <- rmvrnorm2D(n, 0, 0, 1, 1, 0.9) %>% 
  as.data.frame() %>% 
  mutate(chain=4,
         iter=seq_along(chain))

chains_stacked <- rbind.data.frame(chain1, chain2, chain3, chain4) %>% 
  mutate(chain=as.factor(chain))
chains_stacked1 <- chains_stacked %>% 
  select(-iter)

full_data <- chains_stacked1
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")
nsim <- 1000
mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}

# 
# # use Vehtari et al. script to calculate Rhat
# chains_wide1 <- chains_stacked %>% 
#   pivot_wider(values_from = c("V1"), names_from = c("chain"), id_cols = "iter")
# rhat_rfun(as.matrix(chains_wide1[, 2:5]))
# chains_wide2 <- chains_stacked %>% 
#   pivot_wider(values_from = c("V2"), names_from = c("chain"), id_cols = "iter")
# rhat_rfun(as.matrix(chains_wide2[, 2:5]))
# 
# # Use Vehtari et al. to calculate ess
# ess_rfun(as.matrix(chains_wide1[, 2:5]))
# ess_rfun(as.matrix(chains_wide2[, 2:5]))
# ess_bulk(as.matrix(chains_wide1[, 2:5]))
# ess_bulk(as.matrix(chains_wide2[, 2:5]))
# ess_tail(as.matrix(chains_wide1[, 2:5]))
# ess_tail(as.matrix(chains_wide2[, 2:5]))
# ess_quantile(as.matrix(chains_wide1[, 2:5]), 0.95)
# ess_tail(as.matrix(chains_wide2[, 2:5]))

# Try monitor functionality -- split chains first
half_iter <- max(chains_stacked$iter) / 2
m_samples <- array(dim = c(half_iter,
                           2*n_distinct(chains_stacked$chain),
                           2))
k <- 1
chains_stacked_first <- chains_stacked %>% 
  filter(iter<=half_iter)
chains_stacked_second <- chains_stacked %>% 
  filter(iter>half_iter)

for(i in 1:half_iter){
  temp <- chains_stacked_first %>% 
    filter(iter==i)
  for(j in 1:4){
    temp1 <- temp %>%
      filter(chain==j)
    m_samples[i, j, ] <- as.numeric(temp1[1, 1:2])
    k <- k + 1
  }
}

for(i in 1:half_iter){
  temp <- chains_stacked_second %>% 
    filter(iter==(i + 500))
  for(j in 5:8){
    temp1 <- temp %>%
      filter(chain==(j - 4))
    m_samples[i, j, ] <- as.numeric(temp1[1, 1:2])
    k <- k + 1
  }
}

g <- 
  qplot(4*colMeans(mAccuracy)) +
  xlab(TeX("$R*$")) +
  xlim(0.9, NA) +
  geom_vline(xintercept = 1, linetype=2) +
  ylab("Count") +
  theme(text=element_text(size=14, colour="black"))
ggsave("../output/bivariate.pdf", g, width = 8, height = 6)
mon <- monitor(m_samples)

```


# 250-dimensional multivariate normal
Fit model in Stan with 10000 iterations
```{r}
N <- 250
A <- rWishart(1, 250, diag(N))[,,1]
saveRDS(A, "../output/A_matrix.rds")
model <- stan_model("mvt_250.stan")
fit <- sampling(model, data=list(N=N, A=A), iter=10000, chains=4, thin=5)
print(fit)
# saveRDS(fit, "../output/mvt_fit_10000.rds")
fit <- readRDS("../output/mvt_fit_10000.rds")
full_data <- rstan::extract(fit, permuted=F)
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = 252)
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:251] <- full_data[j, i, ]
    m_flattened[k, 252] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V252) %>% 
  mutate(chain=as.factor(chain))

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

nsim <- 1000
mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}

a_df <- tibble(samples_10000=colMeans(mAccuracy)/(1/n_distinct(training_data$chain)))

importance <- 100-varImp(gbmFit1)$importance$Overall
rhat <- summary(fit)$summary[, 10]
ess <- summary(fit)$summary[, 9]
plot(importance, rhat)
plot(importance, ess)
b_df <- tibble(importance, ess, rhat, lp=c(rep(0, 250), 1))
sum(rhat<1.01)
sum(ess>400)
gc <- 
  b_df %>%
  ggplot(aes(importance, ess)) +
  geom_jitter(width=1, aes(shape=as.factor(lp))) +
  geom_smooth(span=1) +
  xlab("Variable importance") +
  ylab("ESS") +
  ggtitle("C.") +
  theme(legend.position = "none")

gb <- b_df %>%
  ggplot(aes(importance, rhat)) +
  geom_jitter(width = 1, aes(shape=as.factor(lp))) +
  geom_smooth(span=1) +
  xlab("Variable importance") +
  ylab(TeX("$\\hat{R}$")) +
  ggtitle("B.") +
  theme(legend.position = "none")
```

With 400 iterations
```{r}
fit <- sampling(model, data=list(N=N, A=A), iter=400, chains=4)
print(fit)
# saveRDS(fit, "../output/mvt_fit_400.rds")
fit <- readRDS("../output/mvt_fit_400.rds")

full_data <- rstan::extract(fit, permuted=F)
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = 252)
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:251] <- full_data[j, i, ]
    m_flattened[k, 252] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V252) %>% 
  mutate(chain=as.factor(chain))

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}

a_df <- a_df %>%
  mutate(samples_400=colMeans(mAccuracy)/(1/n_distinct(training_data$chain)))
```

```{r}
model <- stan_model("mvt_250_ncp.stan")
fit <- sampling(model, data=list(N=N, A=A), iter=10000, chains=4, thin=5)
print(fit)
# saveRDS(fit, "../output/mvt_ncp_fit_10000.rds")
fit <- readRDS("../output/mvt_ncp_fit_10000.rds")

full_data <- rstan::extract(fit, permuted=F)
nparams <- dim(full_data)[3]
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = (nparams + 1))
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams + 1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V502) %>% 
  mutate(chain=as.factor(chain))
n_distinct(m_flattened$chain)

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}

a_df <- a_df %>%
  mutate(samples_10000_ncp=colMeans(mAccuracy)/(1/n_distinct(training_data$chain)))

saveRDS(a_df, "../output/mvt_fit_all_accuracy.rds")

ga <- 
  a_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=fct_relevel(as.factor(variable), "samples_400", "samples_10000", "samples_10000_ncp"))) +
  geom_histogram(position="identity", alpha=0.8, bins=20) +
  scale_fill_grey("# of samples", labels=c("400", "10000, cp", "10000, ncp")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  theme(text = element_text(size=14, colour="black"),
        legend.position = c(0.8, 0.8)) +
  ggtitle("A.")

pdf("../output/mvt_three.pdf", width = 12, height = 8)
multiplot(ga, gb, gc, cols = 3)
dev.off()
```

## Aki Vehtari's suggestion: try measure for similar numbers of iterations to posterior
Centered
```{r}
N <- 250
A <- readRDS("../output/A_matrix.rds")
model <- stan_model("mvt_250.stan")
fit <- sampling(model, data=list(N=N, A=A), iter=500, chains=4, thin=1)
saveRDS(fit, "../output/mvt_fit_500.rds")
fit <- readRDS("../output/mvt_fit_500.rds")

full_data <- rstan::extract(fit, permuted=F, inc_warmup=F)
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = 252)
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:251] <- full_data[j, i, ]
    m_flattened[k, 252] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V252) %>% 
  mutate(chain=as.factor(chain))

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

nsim <- 1000
mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}

a_df <- tibble(samples_cp=colMeans(mAccuracy)/(1/n_distinct(training_data$chain)))

mon <- monitor(fit)
mean(mon$Rhat>1.01)
mean(mon$Bulk_ESS<400)
mean(mon$Tail_ESS<400)
```

Non-centered
```{r}
A <- readRDS("../output/A_matrix.rds")
model <- stan_model("mvt_250_ncp.stan")
fit <- sampling(model, data=list(N=N, A=A), iter=500, chains=4, thin=1)
saveRDS(fit, "../output/mvt_ncp_fit_500.rds")
fit <- readRDS("../output/mvt_ncp_fit_500.rds")

full_data <- rstan::extract(fit, permuted=F, inc_warmup=F)
nparams <- dim(full_data)[3]
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = (nparams + 1))
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams + 1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V502) %>% 
  mutate(chain=as.factor(chain))
n_distinct(m_flattened$chain)

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}

a_df <- a_df %>%
  mutate(samples_ncp=colMeans(mAccuracy)/(1/n_distinct(training_data$chain)))
saveRDS(a_df, "../output/mvt_500_all.rds")
g <- 
  a_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8, bins=20) +
  scale_fill_grey("Model", labels=c("centered", "noncentered")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  theme(text = element_text(size=14, colour="black"),
        legend.position = c(0.8, 0.8))
ggsave("../output/wide_data.pdf", g, width = 12, height = 8)

mon <- monitor(fit)
mean(mon$Rhat>1.01)
mean(mon$Bulk_ESS<400)
mean(mon$Tail_ESS<400)
```

# Wide dataset - standard normal
```{r}
N <- 10000
A <- diag(nrow=N)
model <- stan_model("mvt_wide.stan")
fit <- sampling(model, data=list(N=N), iter=400, chains=4)
saveRDS(fit, "../output/mvt_wide_400.rds")

full_data <- rstan::extract(fit, permuted=F, inc_warmup=F)
nparams <- dim(full_data)[3]
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = (nparams + 1))
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams + 1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame()
colnames(m_flattened)[nparams + 1] <- "chain"
m_flattened <- m_flattened %>% 
  mutate(chain=as.factor(chain))
n_distinct(m_flattened$chain)

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

nsim <- 1000
mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}

a_df <- tibble(samples_400=colMeans(mAccuracy)/(1/n_distinct(training_data$chain)))
saveRDS(a_df, "../output/mvt_wide_400_summary.rds")
mon <- monitor(fit)
mean(mon$Rhat>1.01)
mean(mon$Bulk_ESS<400)
mean(mon$Tail_ESS<400)
```

```{r}
N <- 10000
A <- diag(nrow=N)
model <- stan_model("mvt_wide.stan")
fit <- sampling(model, data=list(N=N), iter=1000, chains=4)
saveRDS(fit, "../output/mvt_wide_1000.rds")

full_data <- rstan::extract(fit, permuted=F, inc_warmup=F)
nparams <- dim(full_data)[3]
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = (nparams + 1))
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams + 1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame()
colnames(m_flattened)[nparams + 1] <- "chain"
m_flattened <- m_flattened %>% 
  mutate(chain=as.factor(chain))
n_distinct(m_flattened$chain)

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

nsim <- 1000
mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}

a_df <- a_df %>%
  tibble(samples_1000=colMeans(mAccuracy)/(1/n_distinct(training_data$chain)))
saveRDS(a_df, "../output/mvt_wide_summaries.rds")
mon <- monitor(fit)
mean(mon$Rhat>1.01)
mean(mon$Bulk_ESS<400)
mean(mon$Tail_ESS<400)

g <- 
  a_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8, bins=20) +
  scale_fill_grey("Model", labels=c("400", "1000")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  theme(text = element_text(size=14, colour="black"),
        legend.position = c(0.8, 0.8))
ggsave("../output/very_wide_data.pdf", g, width = 12, height = 8)
```



# Cauchy example

Generate R* distribution from nominal Cauchy fit
```{r}
nsim <- 1000
fit_nom <- stan(file = 'cauchy_nom.stan', seed = 7878, refresh = 0)
full_data <- rstan::extract(fit_nom, permuted=F)
nparams <- 52
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = (nparams + 1))
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams+1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V53) %>% 
  mutate(chain=as.factor(chain))
n_distinct(m_flattened$chain)

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}
mAccuracy_nom <- mAccuracy
```


Generate R* distribution from alternative Cauchy parameterisation
```{r}
fit_alt1 <- stan(file = 'cauchy_alt_1.stan', seed = 7878, refresh = 0)
full_data <- rstan::extract(fit_alt1, permuted=F)
nparams <- dim(full_data)[3]
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = (nparams + 1))
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams+1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V153) %>% 
  mutate(chain=as.factor(chain))
n_distinct(m_flattened$chain)

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}
mAccuracy_alt_1 <- mAccuracy
```



```{r}
a_df <- tibble(nominal=colMeans(mAccuracy_nom)/(1/n_distinct(training_data$chain)),
               alt=colMeans(mAccuracy_alt_1)/(1/n_distinct(training_data$chain)))
 
g <-
  a_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8) +
  scale_fill_grey("Parameterisation", labels=c("nominal", "alternative")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  theme(text = element_text(size=14, colour="black"),
        legend.position = "none")
g

ggsave("../output/cauchy.pdf", g, width = 8, height = 6)
```

## More iterations
Nominal model with 10000 iterations and thinning by 10
```{r}
fit_nom <- stan(file = 'cauchy_nom.stan', seed = 7878, refresh = 0, iter = 10000,
                thin = 5)
full_data <- rstan::extract(fit_nom, permuted=F)
nparams <- 52
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = (nparams + 1))
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams+1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V53) %>% 
  mutate(chain=as.factor(chain))
n_distinct(m_flattened$chain)

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

nsim <- 1000
mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}
hist(colMeans(mAccuracy)/(1/n_distinct(training_data$chain)))
mAccuracy_nom <- mAccuracy
```

Alternative model with 10000 iterations and thinning by 10
```{r}
fit_alt1 <- stan(file = 'cauchy_alt_1.stan', seed = 7878, refresh = 0, iter = 10000,
                thin = 5)
full_data <- rstan::extract(fit_alt1, permuted=F)
nparams <- dim(full_data)[3]
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = (nparams + 1))
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams+1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V153) %>% 
  mutate(chain=as.factor(chain))
n_distinct(m_flattened$chain)

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}
mAccuracy_alt_1 <- mAccuracy
```

Plot
```{r}
b_df <- tibble(nominal=colMeans(mAccuracy_nom)/(1/n_distinct(training_data$chain)),
               alt=colMeans(mAccuracy_alt_1)/(1/n_distinct(training_data$chain)))
 
g1 <-
  a_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8) +
  scale_fill_grey("Parameterisation", labels=c("nominal", "alternative")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  theme(text = element_text(size=14, colour="black"),
        legend.position = "none") +
  ggtitle("A.")
g1

g2 <-
  b_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8) +
  scale_fill_grey("Parameterisation", labels=c("nominal", "alternative")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  theme(text = element_text(size=14, colour="black")) +
  ggtitle("B.")
g2

pdf("../output/cauchy.pdf", width = 12, height = 6)
multiplot(g1, g2, cols = 2)
dev.off()
```

# Eight schools
```{r}
source("eight_schools.data.R")
eight_schools <- list(J=J, y=y, sigma=sigma)
model_cp <- stan_model("eight_schools_cp.stan")
model_ncp <- stan_model("eight_schools_ncp.stan")
```

Run models
```{r}
fit_cp <- sampling(
  model_cp, data = eight_schools,
  iter = 2000, chains = 4, seed = 483892929, refresh = 0,
  control = list(adapt_delta = 0.95)
)

fit_ncp <- sampling(
  model_ncp, data = eight_schools,
  iter = 2000, chains = 4, seed = 483892929, refresh = 0,
  control = list(adapt_delta = 0.95)
)
```

## Break each chain into two then make flattened data
Centered
```{r}
full_data_all <- rstan::extract(fit_cp, permuted=F)
full_data <- array(dim=c(500, 8, 11))
k <- 1
for(i in 1:4){
  first_half <- full_data_all[1:500, i, ]
  second_half <- full_data_all[501:1000, i, ]
  full_data[, k, ] <- first_half
  k <- k + 1
  full_data[, k, ] <- second_half
  k <- k + 1
}
  

m_flattened <- matrix(nrow = nrow(full_data) * 8, ncol = 12)
k <- 1
for(i in 1:8){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:11] <- full_data[j, i, ]
    m_flattened[k, 12] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V12) %>% 
  mutate(chain=as.factor(chain))

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

nsim <- 1000
mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}
mAccuracy_centered <- mAccuracy
```

Noncentered
```{r}
full_data_all <- rstan::extract(fit_ncp, permuted=F)
nparams <- 19
full_data <- array(dim=c(500, 8, nparams))
k <- 1
for(i in 1:4){
  first_half <- full_data_all[1:500, i, ]
  second_half <- full_data_all[501:1000, i, ]
  full_data[, k, ] <- first_half
  k <- k + 1
  full_data[, k, ] <- second_half
  k <- k + 1
}
  

m_flattened <- matrix(nrow = nrow(full_data) * 8, ncol = (nparams+1))
k <- 1
for(i in 1:8){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams+1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V20) %>% 
  mutate(chain=as.factor(chain))

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}
mAccuracy_noncentered <- mAccuracy
```

Store both
```{r}
c_df <- tibble(centered=colMeans(mAccuracy_centered)/0.125,
       non_centered=colMeans(mAccuracy_noncentered)/0.125)
```

## Trying with original data (i.e. 4 chains)
```{r}
full_data <- rstan::extract(fit_cp, permuted=F)
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = 12)
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:11] <- full_data[j, i, ]
    m_flattened[k, 12] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V12) %>% 
  mutate(chain=as.factor(chain))

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

nsim <- 1000
mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}
mAccuracy_centered <- mAccuracy

full_data <- rstan::extract(fit_ncp, permuted=F)
k <- 1
m_flattened <- matrix(nrow = nrow(full_data) * 4, ncol = (nparams+1))
k <- 1
for(i in 1:4){
  for(j in 1:nrow(full_data)){
    m_flattened[k, 1:nparams] <- full_data[j, i, ]
    m_flattened[k, (nparams+1)] <- i
    k <- k + 1
  }
}
m_flattened <- m_flattened %>% 
  as.data.frame() %>% 
  rename(chain=V20) %>% 
  mutate(chain=as.factor(chain))

full_data <- m_flattened
rand_samples <- sample(1:nrow(full_data), 0.7 * nrow(full_data))
training_data <- full_data[rand_samples, ]
testing_data <- full_data[-rand_samples, ]
gbmFit1 <- train(chain ~ ., data = training_data, 
                 method = "gbm",
                 trControl = trainControl(method = 'none'), 
                    tuneGrid = caretGrid, verbose=FALSE)

plda <- predict(object=gbmFit1, newdata=testing_data)
tibble(predicted=plda, actual=testing_data$chain) %>% 
  mutate(correct=if_else(predicted==actual, 1, 0)) %>% 
  summarise(mean(correct))
varImp(gbmFit1)

plda <- predict(object=gbmFit1, newdata=testing_data, type = "prob")

mAccuracy <- matrix(nrow = nrow(plda),
                    ncol = nsim)
for(i in 1:nsim){
  for(j in 1:nrow(plda)){
    mAccuracy[j, i] <- if_else(which(rmultinom(1, 1, prob = plda[j, ])==1)==testing_data$chain[j], 1, 0)
  }
}
mAccuracy_noncentered <- mAccuracy
```

```{r}
d_df <- tibble(centered=colMeans(mAccuracy_centered)/(1/n_distinct(training_data$chain)),
       non_centered=colMeans(mAccuracy_noncentered)/(1/n_distinct(training_data$chain)))
```

Plot two R* distributions
```{r}
g1 <- 
  c_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8) +
  xlim(0.78, 1.6) +
  scale_fill_grey("Series", labels=c("centered", "non-centered")) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  ggtitle("A.") +
  theme(text = element_text(size=14, colour="black"),
        legend.position = "none")
g2 <- 
  d_df %>% 
  melt() %>% 
  ggplot(aes(x=value, fill=as.factor(variable))) +
  geom_histogram(position="identity", alpha=0.8) +
  scale_fill_grey("Series", labels=c("centered", "non-centered")) +
  xlim(0.78, 1.6) +
  xlab(TeX("$R*$")) +
  ylab("Count") +
  geom_vline(xintercept = 1, linetype=2) +
  ggtitle("B.") +
  theme(text = element_text(size=14, colour="black"))
a_df <- readRDS("../output/eight_schools_replicates_4_index.rds")

pdf("../output/eight_schools.pdf", width = 12, height = 6)
multiplot(g1, g2, cols = 2)
dev.off()
```



